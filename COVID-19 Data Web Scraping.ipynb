{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***WEB SCRAPING FOR CORONAVIRUS DATA***\n",
    ">### *Felipe Solares*\n",
    ">### 30/03/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***About***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Web Scraping project using Python 3.7 developed by Felipe Solares da Silva. This is part of his `professional portfolio` and if you want to see more projects like this, go and check my portfolio at https://github.com/fsolares/professional-portfolio.\n",
    "\n",
    "Contact: solares.fs@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Project Purpose***\n",
    ">Perform a web scraping, mining for `COVID-19 data` from Brazilian's Health Ministries site to build my own database fro futher analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Installing and Importing Essential Packages and Modules\n",
    ">To this project, we're going to use `BeautifulSoup`, `Selenium` and `Regex`. \n",
    ">- Beautiful Soup is a Python library for pulling data out of HTML and XML files. \n",
    ">- Selenium is a tool designed to automate Web Browser. It is able to click at specific form buttons, input information in text fields and extract the DOM elements for browser HTML code.\n",
    ">- Regex or Regular Expressions is a sequence of characters that define a search pattern.\n",
    "\n",
    "**Just run the cell bellow to install all packages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\solar\\anaconda3\\lib\\site-packages (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\solar\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.0.1)\n",
      "Collecting selenium\n",
      "  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\solar\\anaconda3\\lib\\site-packages (from selenium) (1.25.10)\n",
      "Installing collected packages: selenium\n",
      "Successfully installed selenium-3.141.0\n"
     ]
    }
   ],
   "source": [
    "# Installing BeautifulSoup\n",
    "!pip install beautifulsoup4\n",
    "# Installing Selenium\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING!**\n",
    ">Selenium requires a driver to interface with the chosen browser. `Firefox`, for example, requires geckodriver, which needs to be installed before the below examples can be run. In this the browser tha we choose was `Google Chrome`.\n",
    "\n",
    "**Access this wonderful tutorial https://selenium-python.readthedocs.io/installation.html if you want more information.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1 - Installing and Configuring the Webdriver for Google Chrome\n",
    ">First, we have to find out the version of the `Google Chrome web browser`. Then we will download the web driver for google chrome through the link below.\n",
    "https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "When we access the site, we will come across different versions for download ... Just choose the one corresponding to your version of google chrome.\n",
    "\n",
    ">Once downloaded, unzip the file into a folder at the `root directory` C:/. Name it as you want, but it is important to save it in the root directory. \n",
    "On my PC it looked like this: `C:\\webdrivers` and inside this folder a single file that was contained in the zip called `chromedriver.exe`.\n",
    "\n",
    ">To complete the configuration, simply navigate to the control panel to place the web driver in your `PATH` (system variables). Seriously! Make sure it’s in your `PATH`.\n",
    "\n",
    "**Finally! Just run the cell bellow and let's scrape.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Building a Web Scraper\n",
    "\n",
    ">The main goal here was to create a Web scraper class to compile all future methods required to scrape the target web page. In the first version of the Brazilian COVID-19 website, the only data provided was a table with daily information about: occurrences, deaths, and lethality by state. Later on, the new versions came and besides the table, they bring a CSV file with all data since day one. \n",
    "\n",
    "\n",
    ">Every day at 7:00 PM the website was updated and with it, new data to be scraped. So we opened the shell, search for the `file.py`, and let the magic happen! But, in order to guarantee that our code worked as we expected, we had to take care of some issues like:\n",
    "    1. Check if the website is online.\n",
    "    2. Check if the website was updated.\n",
    "    3. Check if we already have the file that we'll download.\n",
    "    4. Move the recently download `CSV` file to another directory.\n",
    "    5. Organize, Clean and Export the data scraped.\n",
    "\n",
    "**Therefore, for a better understanding of  how each item in the list above was resolved, just look in the next cell the description and the code of each created method.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Webscraper:\n",
    "    \n",
    "    # The Web scraper class, compile all functions \n",
    "    # required to scrape the target web page.\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.url = 'https://covid.saude.gov.br/'\n",
    "        self.datenow = datetime.datetime.now()\n",
    "        self.option = Options()\n",
    "        self.option.add_experimental_option(\"prefs\", {\"download.default_directory\": \"/path/to/download/dir\",\n",
    "                                                      \"download.prompt_for_download\": False})\n",
    "        self.option.headless = True # (Comment this line if you want to see all process running)\n",
    "        \n",
    "        # This line will work only if you are using Google Chrome.\n",
    "        self.driver = webdriver.Chrome(\"C:/webdrivers/chromedriver.exe\", options=self.option) \n",
    "                                                                                               \n",
    "                                                                                            \n",
    "        \n",
    "    \n",
    "    def get_File(self):\n",
    "        \n",
    "        # The get_File method, checks if the link is active, \n",
    "        # download the file, rename it and change its directory.\n",
    "        \n",
    "        import os\n",
    "        \n",
    "        self.driver.command_executor._commands[\"send_command\"] = (\"POST\",\n",
    "                                                                  '/session/$sessionId/chromium/send_command')\n",
    "        \n",
    "        params = {'cmd':'Page.setDownloadBehavior', 'params': {'behavior': 'allow', \n",
    "                                                               'downloadPath': 'C:\\\\Users\\\\solar\\\\Downloads'}}\n",
    "        \n",
    "        command_result = self.driver.execute(\"send_command\", params)\n",
    "        \n",
    "        \n",
    "        print('Dowloading CSV file to C:/Users/solar/Downloads...', end='\\n\\n')\n",
    "        \n",
    "        self.driver.find_element_by_xpath('//html/body/app-root/ion-app/ion-router-outlet'\n",
    "                                          '/app-home/ion-content/div[1]/div[2]/ion-button').click()\n",
    "        \n",
    "        time.sleep(5)\n",
    "        for file in os.listdir(r'C:\\\\Users\\\\solar\\\\Downloads'):\n",
    "            if file.endswith('.csv'):\n",
    "                filename = self.datenow.strftime('%d%m%Y') + '.csv'\n",
    "                dirname = os.path.join(r'C:\\\\Users\\\\solar\\\\Downloads', file)\n",
    "                newdirname = 'F:\\\\FCD\\\\Covid19\\\\SUS_csv\\\\BRnCov19_' + filename\n",
    "                if os.path.exists(newdirname):\n",
    "                    print('  File already exists...', end='\\n\\n')\n",
    "                else:\n",
    "                    print('  Moving the file to F:\\FCD\\Covid19\\SUS_csv...', end='\\n\\n')\n",
    "                    shutil.move(dirname, newdirname)\n",
    "                    self.driver.quit()             \n",
    "\n",
    "\n",
    "    def check_Url(self):\n",
    "        \n",
    "        # The check_Url() method tests an url using the requests package,\n",
    "        # returning True or False depending of what response \n",
    "        # retrieved from the web page.\n",
    "        \n",
    "        print()\n",
    "        print('Establishing Connection...', end='\\n\\n')\n",
    "        time.sleep(2)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(self.url)\n",
    "            print('  We are online!', end='\\n\\n')\n",
    "            \n",
    "        except Exception:\n",
    "            print('An error has occurred. Please check your internet connection ' +\n",
    "                  'or check if the url still exists.')\n",
    "        \n",
    "        else:\n",
    "            return True\n",
    "    \n",
    "     \n",
    "    def check_File(self, ptrn):\n",
    "        \n",
    "        # The check_File() method checks whether the specified file exists or not\n",
    "        # returning True or False.\n",
    "        \n",
    "        print()\n",
    "        print('Checking current release...', end='\\n\\n')\n",
    "        time.sleep(2)\n",
    "        \n",
    "        cut = re.sub('/', '', ptrn)\n",
    "        filename = 'BRcov19_' + cut + '.csv'\n",
    "        \n",
    "        if path.exists(filename):\n",
    "            print(f'  You already have this version!. The last update was on {ptrn}.')\n",
    "            print('  Try again later!')\n",
    "            return True\n",
    "        return False\n",
    "            \n",
    "    def check_Rls(self):\n",
    "        \n",
    "        # The check_Rls() method search for the date and time\n",
    "        # of the last release, returning this value as a formatted\n",
    "        # string.\n",
    "        \n",
    "        self.driver.get(self.url)\n",
    "        time.sleep(5)\n",
    "        \n",
    "        release = self.driver.find_element_by_xpath('//html/body/app-root/ion-app/ion-router-outlet'\n",
    "                                                    '/app-home/ion-content/div[1]/div[1]/div[3]/span').text\n",
    "        \n",
    "        pattern = re.findall('(\\d{2}\\/\\d{2}\\/\\d{4})', release)\n",
    "        \n",
    "        return ws.check_File(pattern[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_Html(self):\n",
    "        \n",
    "        # The get_Html() method search for a specific \n",
    "        # element using xpath, returning a parsed html element \n",
    "        # transformed by BeautifulSoup.\n",
    "        \n",
    "        print('Scraping Data...', end='\\n\\n')\n",
    "        time.sleep(2)\n",
    "\n",
    "        element = self.driver.find_element_by_xpath('//html/body/app-root/ion-app/ion-router-outlet/app-home'\n",
    "                                                    '/ion-content/painel-geral-component/div/div[2]/div[2]'\n",
    "                                                    '/div[1]/lista-itens-component/div[2]')\n",
    "        \n",
    "        html_content = element.get_attribute('outerHTML')\n",
    "\n",
    "        #self.driver.quit()\n",
    "        \n",
    "        soup = bs(html_content, 'html.parser') \n",
    "\n",
    "        return soup\n",
    "    \n",
    "    def get_Content(self, pars_elem):\n",
    "        \n",
    "        # The get_Content(pars_elem) method receives a parsed html element \n",
    "        # as input and search for a specific content\n",
    "        # using html tags, extract the target content and returns a data frame\n",
    "        # with the organized data.\n",
    "        \n",
    "        print('Cleaning Content...',end='\\n\\n')\n",
    "        time.sleep(2)\n",
    "  \n",
    "        state = []\n",
    "        confirmed = []\n",
    "        death = []\n",
    "        incidence = []\n",
    "        lethality = []\n",
    "        \n",
    "        for idx, c in enumerate(pars_elem.find_all(class_ = 'lb-nome')):\n",
    "            if idx in range(0, 131, 5):      # In this loop, we're taking all matchs returned by the \n",
    "                state.append(c.text)         # search method, extracting the content inside the\n",
    "            elif idx in range(1, 132, 5):    # tags(using string methods) and \n",
    "                confirmed.append(c.text)     # allocating them in a specific list.\n",
    "            elif idx in range(2, 133, 5):\n",
    "                death.append(c.text)\n",
    "            elif idx in range(3, 134, 5):\n",
    "                incidence.append(c.text)\n",
    "            else:\n",
    "                lethality.append(c.text)\n",
    "        \n",
    "        \n",
    "        print('Creating Data Frame...',end='\\n\\n')\n",
    "        \n",
    "        # Zipping through the lists creating an organized data frame.\n",
    "        br_coronavirus = pd.DataFrame(list(zip(state, confirmed, death, incidence, lethality)), \n",
    "                             columns = ['State', 'Confirmed_Cases', 'Deaths', 'Incidence', 'Lethality']) \n",
    "                                                                                                                      \n",
    "                                                                                           \n",
    "        br_coronavirus['time_stamp'] = self.datenow.strftime('%d/%m/%Y')\n",
    "        \n",
    "        return br_coronavirus\n",
    "    \n",
    "    def export(self, inframe):\n",
    "        \n",
    "        # The export() method receives a data frame as input and\n",
    "        # exports the data to a CSV file, storing it in your personal database.\n",
    "        \n",
    "        print('Exporting File...',end='\\n\\n')\n",
    "        time.sleep(2)        \n",
    "        \n",
    "        filename = 'BRcov19_' + self.datenow.strftime('%d%m%Y') + '.csv'\n",
    "        inframe.to_csv(filename, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - The Simple script\n",
    ">Let's understand the logic that starts our web scraper and automates our tasks. The first line starts an infinite loop, then we call the `Webscraper()` assigning it to an object. At line 3, we check if the web page is online if `TRUE` we pass to line 4 and check the release (if is up to date!), if `FALSE` we stop the operation.\n",
    "\n",
    ">If everything goes as we expected, the scraper gets the job done, and voilá! New data to store and analyze.\n",
    "\n",
    "**Check the geospatial analysis performed using the data scraped using this project at https://github.com/fsolares/Python-COVID-19_Geospatial_Analysis**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ws = Webscraper()\n",
    "    if ws.check_Url():\n",
    "        if ws.check_Rls():\n",
    "            break\n",
    "        else:\n",
    "            html_element = ws.get_Html()\n",
    "            br_coronavirus = ws.get_Content(html_element)\n",
    "            ws.export(br_coronavirus)\n",
    "            ws.get_File()\n",
    "            print('Done!')\n",
    "            break        \n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCLAIMER**\n",
    ">Perhaps, when you are looking at this project, the site has probably changed its layout...Don't panic!\n",
    "Just find among all new possibilities how to solve the problem and get job done!\n",
    "Good Luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That’s all for today! If you’d like to take a look at another project, fell free to check-out my github portfolio at https://github.com/fsolares/professional-portfolio\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
